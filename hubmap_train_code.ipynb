{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import sys\n","sys.path.append(\"./detection\")"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import os\n","import numpy as np\n","import torch\n","from PIL import Image\n","from pathlib import Path\n","import cv2\n","from engine import train_one_epoch, evaluate\n","from data_process import img_showmask\n","import utils\n","import torch.utils.data as data\n","from tqdm.auto import tqdm\n","import matplotlib.pyplot as plt\n","\n","\n","class PennFudanDataset(torch.utils.data.Dataset):\n","\n","    def __init__(self, root, transforms):\n","        self.root = root\n","        self.transforms = transforms\n","        # load all image files, sorting them to\n","        # ensure that they are aligned\n","        self.imgs = list(sorted(os.listdir(os.path.join(root, \"image\"))))\n","        self.masks = list(sorted(os.listdir(os.path.join(root, \"mask\"))))\n","        # self.imgs = list(sorted(os.listdir(os.path.join(root, \"ori_tif\"))))\n","        # self.masks = list(sorted(os.listdir(os.path.join(root, \"ori_tif\"))))\n","\n","\n","    def __getitem__(self, idx):\n","        # load images and masks\n","        img_path = os.path.join(self.root, \"image\", self.imgs[idx])\n","        mask_path = os.path.join(self.root, \"mask\", self.masks[idx])\n","        # print(img_path)\n","        # img_path = self.imgs[idx]\n","        # mask_path = self.masks[idx]\n","        img = Image.open(img_path).convert(\"RGB\")\n","        # note that we haven't converted the mask to RGB,\n","        # because each color corresponds to a different instance\n","        # with 0 being background\n","        mask = Image.open(mask_path).convert('L')\n","        # convert the PIL Image into a numpy array\n","        mask = np.array(mask)\n","        # instances are encoded as different colors\n","        obj_ids = np.unique(mask)\n","        # first id is the background, so remove it\n","        \n","        obj_ids = obj_ids[1:]\n","        # print((obj_ids[:, None, None]).shape)\n","        # print(obj_ids)\n","        # split the color-encoded mask into a set\n","        # of binary masks\n","        masks = mask == obj_ids[:, None, None]\n","        \n","        # print(masks)\n","        \n","\n","        # get bounding box coordinates for each mask\n","        num_objs = len(obj_ids)\n","        boxes = []\n","        for i in range(num_objs):\n","            pos = np.nonzero(masks[i])\n","            xmin = np.min(pos[1])\n","            xmax = np.max(pos[1])\n","            ymin = np.min(pos[0])\n","            ymax = np.max(pos[0])\n","            boxes.append([xmin, ymin, xmax, ymax])\n","\n","        # convert everything into a torch.Tensor\n","        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","        # there is only one class\n","        labels = torch.ones((num_objs,), dtype=torch.int64)\n","        masks = torch.as_tensor(masks, dtype=torch.uint8)\n","        image_id = torch.tensor([idx])\n","\n","        \n","        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n","    \n","        # suppose all instances are not crowd\n","        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n","\n","        target = {}\n","        target[\"boxes\"] = boxes\n","        target[\"labels\"] = labels\n","        target[\"masks\"] = masks\n","        target[\"image_id\"] = image_id\n","        target[\"area\"] = area\n","        target[\"iscrowd\"] = iscrowd\n","\n","        path = {}\n","        path[\"mask_path\"] = mask_path\n","        path[\"image_path\"] = img_path\n","\n","        if self.transforms is not None:\n","            img, target = self.transforms(img, target)\n","\n","        return img, target, path\n","\n","    def __len__(self):\n","        return len(self.imgs)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import torchvision\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","\n","# load a model pre-trained on COCO\n","model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n","\n","# replace the classifier with a new one, that has\n","# num_classes which is user-defined\n","num_classes = 2  # 1 class (person) + background\n","# get number of input features for the classifier\n","in_features = model.roi_heads.box_predictor.cls_score.in_features\n","# replace the pre-trained head with a new one\n","model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["import torchvision\n","from torchvision.models.detection import FasterRCNN\n","from torchvision.models.detection.rpn import AnchorGenerator\n","\n","# load a pre-trained model for classification and return\n","# only the features\n","backbone = torchvision.models.mobilenet_v2(weights=\"DEFAULT\").features\n","# FasterRCNN needs to know the number of\n","# output channels in a backbone. For mobilenet_v2, it's 1280\n","# so we need to add it here\n","backbone.out_channels = 1280\n","\n","# let's make the RPN generate 5 x 3 anchors per spatial\n","# location, with 5 different sizes and 3 different aspect\n","# ratios. We have a Tuple[Tuple[int]] because each feature\n","# map could potentially have different sizes and\n","# aspect ratios\n","anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n","                                   aspect_ratios=((0.5, 1.0, 2.0),))\n","\n","# let's define what are the feature maps that we will\n","# use to perform the region of interest cropping, as well as\n","# the size of the crop after rescaling.\n","# if your backbone returns a Tensor, featmap_names is expected to\n","# be [0]. More generally, the backbone should return an\n","# OrderedDict[Tensor], and in featmap_names you can choose which\n","# feature maps to use.\n","roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n","                                                output_size=7,\n","                                                sampling_ratio=2)\n","\n","# put the pieces together inside a FasterRCNN model\n","model = FasterRCNN(backbone,\n","                   num_classes=2,\n","                   rpn_anchor_generator=anchor_generator,\n","                   box_roi_pool=roi_pooler)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["import torchvision\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n","\n","\n","def get_model_instance_segmentation(num_classes):\n","    # load an instance segmentation model pre-trained on COCO\n","    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n","\n","    # get number of input features for the classifier\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    # replace the pre-trained head with a new one\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","    # now get the number of input features for the mask classifier\n","    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n","    hidden_layer = 256\n","    # and replace the mask predictor with a new one\n","    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n","                                                       hidden_layer,\n","                                                       num_classes)\n","\n","    return model"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["import transforms as T\n","\n","def get_transform(train):\n","    transforms = []\n","    transforms.append(T.PILToTensor())\n","    transforms.append(T.ConvertImageDtype(torch.float))\n","    if train:\n","        transforms.append(T.RandomHorizontalFlip(0.5))\n","    return T.Compose(transforms)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# from engine import train_one_epoch, evaluate\n","# import utils\n","# import torch.utils.data as data\n","# from tqdm.auto import tqdm\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n","# dataset = PennFudanDataset('./data/hubmap-hacking-the-human-vasculature/train', get_transform(train=True))\n","# data_loader = torch.utils.data.DataLoader(\n","#  dataset, batch_size=2, shuffle=True, num_workers=0,\n","#  collate_fn=utils.collate_fn)\n","# # For Training\n","# images,targets = next(iter(data_loader))\n","# images = list(image for image in images)\n","# targets = [{k: v for k, v in t.items()} for t in targets]\n","# output = model(images,targets)   # Returns losses and detections\n","# # For inference\n","# model.eval()\n","# x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n","# predictions = model(x)           # Returns predictions"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# train on the GPU or on the CPU, if a GPU is not available\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","# our dataset has two classes only - background and person\n","num_classes = 2\n","# use our dataset and defined transformations\n","\n","dataset = PennFudanDataset('./data/hubmap-hacking-the-human-vasculature/train', get_transform(train=True))\n","dataset_test = PennFudanDataset('./data/hubmap-hacking-the-human-vasculature/train', get_transform(train=False))\n","\n","train_size=int(len(dataset)*0.9)\n","validation_size=len(dataset)-train_size\n","dataset,dataset_test=data.random_split(dataset, [train_size,validation_size])\n","\n","\n","# split the dataset in train and test set\n","# indices = torch.randperm(len(dataset)).tolist()\n","# dataset = torch.utils.data.Subset(dataset, indices[:-50])\n","# dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n","\n","# define training and validation data loaders\n","data_loader = torch.utils.data.DataLoader(\n","    dataset, batch_size=2, shuffle=True, num_workers=0,\n","    collate_fn=utils.collate_fn)\n","\n","data_loader_test = torch.utils.data.DataLoader(\n","    dataset_test, batch_size=1, shuffle=False, num_workers=0,\n","    collate_fn=utils.collate_fn)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b6212837243d4ca69ba5acaf47bdb4a5","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: [0]  [  0/730]  eta: 0:30:59  lr: 0.000012  loss: 4.6998 (4.6998)  loss_classifier: 0.7658 (0.7658)  loss_box_reg: 0.3631 (0.3631)  loss_mask: 1.2159 (1.2159)  loss_objectness: 2.2759 (2.2759)  loss_rpn_box_reg: 0.0791 (0.0791)  time: 2.5471  data: 0.0533  max mem: 1644\n","Epoch: [0]  [ 10/730]  eta: 0:06:17  lr: 0.000080  loss: 3.8393 (4.0262)  loss_classifier: 0.7071 (0.6746)  loss_box_reg: 0.1530 (0.1764)  loss_mask: 1.0282 (1.0593)  loss_objectness: 2.0144 (2.0282)  loss_rpn_box_reg: 0.0791 (0.0877)  time: 0.5236  data: 0.0351  max mem: 1927\n","Epoch: [0]  [ 20/730]  eta: 0:05:08  lr: 0.000149  loss: 2.3536 (3.0340)  loss_classifier: 0.4351 (0.5281)  loss_box_reg: 0.1898 (0.2115)  loss_mask: 0.7061 (0.8497)  loss_objectness: 0.7995 (1.3582)  loss_rpn_box_reg: 0.0653 (0.0866)  time: 0.3289  data: 0.0345  max mem: 2025\n","Epoch: [0]  [ 30/730]  eta: 0:04:40  lr: 0.000217  loss: 1.6253 (2.5296)  loss_classifier: 0.3143 (0.4446)  loss_box_reg: 0.2182 (0.2228)  loss_mask: 0.5975 (0.7501)  loss_objectness: 0.4307 (1.0344)  loss_rpn_box_reg: 0.0538 (0.0777)  time: 0.3339  data: 0.0351  max mem: 2025\n","Epoch: [0]  [ 40/730]  eta: 0:04:24  lr: 0.000286  loss: 1.3075 (2.2414)  loss_classifier: 0.2468 (0.4013)  loss_box_reg: 0.1873 (0.2243)  loss_mask: 0.4933 (0.6775)  loss_objectness: 0.2720 (0.8543)  loss_rpn_box_reg: 0.0529 (0.0840)  time: 0.3298  data: 0.0341  max mem: 2025\n","Epoch: [0]  [ 50/730]  eta: 0:04:15  lr: 0.000354  loss: 1.3063 (2.0807)  loss_classifier: 0.2899 (0.3884)  loss_box_reg: 0.2418 (0.2418)  loss_mask: 0.4635 (0.6345)  loss_objectness: 0.2047 (0.7313)  loss_rpn_box_reg: 0.0546 (0.0846)  time: 0.3355  data: 0.0350  max mem: 2025\n","Epoch: [0]  [ 60/730]  eta: 0:04:07  lr: 0.000423  loss: 1.3395 (1.9573)  loss_classifier: 0.3049 (0.3726)  loss_box_reg: 0.3276 (0.2545)  loss_mask: 0.4178 (0.6007)  loss_objectness: 0.2047 (0.6454)  loss_rpn_box_reg: 0.0550 (0.0842)  time: 0.3418  data: 0.0360  max mem: 2025\n","Epoch: [0]  [ 70/730]  eta: 0:04:02  lr: 0.000491  loss: 1.3083 (1.8625)  loss_classifier: 0.3049 (0.3626)  loss_box_reg: 0.3418 (0.2708)  loss_mask: 0.3938 (0.5681)  loss_objectness: 0.1925 (0.5797)  loss_rpn_box_reg: 0.0566 (0.0813)  time: 0.3449  data: 0.0351  max mem: 2178\n","Epoch: [0]  [ 80/730]  eta: 0:03:57  lr: 0.000560  loss: 1.3065 (1.7889)  loss_classifier: 0.3055 (0.3537)  loss_box_reg: 0.3872 (0.2814)  loss_mask: 0.3637 (0.5454)  loss_objectness: 0.1474 (0.5301)  loss_rpn_box_reg: 0.0591 (0.0782)  time: 0.3489  data: 0.0346  max mem: 2178\n","Epoch: [0]  [ 90/730]  eta: 0:03:51  lr: 0.000629  loss: 1.2194 (1.7131)  loss_classifier: 0.2609 (0.3407)  loss_box_reg: 0.3238 (0.2833)  loss_mask: 0.3629 (0.5270)  loss_objectness: 0.1385 (0.4869)  loss_rpn_box_reg: 0.0466 (0.0752)  time: 0.3428  data: 0.0340  max mem: 2178\n","Epoch: [0]  [100/730]  eta: 0:03:47  lr: 0.000697  loss: 1.0786 (1.6608)  loss_classifier: 0.2490 (0.3350)  loss_box_reg: 0.3737 (0.2946)  loss_mask: 0.3438 (0.5083)  loss_objectness: 0.1159 (0.4497)  loss_rpn_box_reg: 0.0466 (0.0732)  time: 0.3436  data: 0.0337  max mem: 2266\n","Epoch: [0]  [110/730]  eta: 0:03:43  lr: 0.000766  loss: 1.1212 (1.6251)  loss_classifier: 0.2621 (0.3314)  loss_box_reg: 0.3737 (0.3064)  loss_mask: 0.3412 (0.4942)  loss_objectness: 0.1208 (0.4219)  loss_rpn_box_reg: 0.0464 (0.0712)  time: 0.3558  data: 0.0347  max mem: 2266\n","Epoch: [0]  [120/730]  eta: 0:03:40  lr: 0.000834  loss: 1.2217 (1.6034)  loss_classifier: 0.2856 (0.3308)  loss_box_reg: 0.4282 (0.3206)  loss_mask: 0.3512 (0.4834)  loss_objectness: 0.1293 (0.3983)  loss_rpn_box_reg: 0.0476 (0.0702)  time: 0.3628  data: 0.0358  max mem: 2266\n","Epoch: [0]  [130/730]  eta: 0:03:36  lr: 0.000903  loss: 1.1990 (1.5691)  loss_classifier: 0.2824 (0.3255)  loss_box_reg: 0.4062 (0.3244)  loss_mask: 0.3498 (0.4731)  loss_objectness: 0.1135 (0.3763)  loss_rpn_box_reg: 0.0476 (0.0698)  time: 0.3577  data: 0.0347  max mem: 2266\n","Epoch: [0]  [140/730]  eta: 0:03:32  lr: 0.000971  loss: 1.1819 (1.5565)  loss_classifier: 0.2699 (0.3228)  loss_box_reg: 0.3691 (0.3327)  loss_mask: 0.3439 (0.4642)  loss_objectness: 0.1236 (0.3653)  loss_rpn_box_reg: 0.0416 (0.0715)  time: 0.3573  data: 0.0346  max mem: 2311\n","Epoch: [0]  [150/730]  eta: 0:03:29  lr: 0.001040  loss: 1.2255 (1.5404)  loss_classifier: 0.2914 (0.3225)  loss_box_reg: 0.3877 (0.3391)  loss_mask: 0.3462 (0.4566)  loss_objectness: 0.1320 (0.3505)  loss_rpn_box_reg: 0.0442 (0.0716)  time: 0.3645  data: 0.0355  max mem: 2311\n","Epoch: [0]  [160/730]  eta: 0:03:25  lr: 0.001108  loss: 1.1498 (1.5126)  loss_classifier: 0.2654 (0.3181)  loss_box_reg: 0.3605 (0.3382)  loss_mask: 0.3440 (0.4496)  loss_objectness: 0.1154 (0.3365)  loss_rpn_box_reg: 0.0442 (0.0703)  time: 0.3575  data: 0.0347  max mem: 2311\n","Epoch: [0]  [170/730]  eta: 0:03:21  lr: 0.001177  loss: 1.0378 (1.4884)  loss_classifier: 0.2356 (0.3138)  loss_box_reg: 0.3217 (0.3393)  loss_mask: 0.3395 (0.4428)  loss_objectness: 0.0878 (0.3236)  loss_rpn_box_reg: 0.0363 (0.0688)  time: 0.3542  data: 0.0339  max mem: 2311\n","Epoch: [0]  [180/730]  eta: 0:03:17  lr: 0.001245  loss: 1.1259 (1.4673)  loss_classifier: 0.2371 (0.3102)  loss_box_reg: 0.3867 (0.3403)  loss_mask: 0.3161 (0.4357)  loss_objectness: 0.0878 (0.3119)  loss_rpn_box_reg: 0.0434 (0.0692)  time: 0.3577  data: 0.0346  max mem: 2311\n","Epoch: [0]  [190/730]  eta: 0:03:14  lr: 0.001314  loss: 1.1327 (1.4502)  loss_classifier: 0.2371 (0.3063)  loss_box_reg: 0.3867 (0.3405)  loss_mask: 0.3228 (0.4341)  loss_objectness: 0.1026 (0.3011)  loss_rpn_box_reg: 0.0434 (0.0683)  time: 0.3579  data: 0.0366  max mem: 2311\n","Epoch: [0]  [200/730]  eta: 0:03:10  lr: 0.001382  loss: 1.1024 (1.4350)  loss_classifier: 0.2366 (0.3041)  loss_box_reg: 0.3848 (0.3427)  loss_mask: 0.3553 (0.4295)  loss_objectness: 0.0919 (0.2912)  loss_rpn_box_reg: 0.0419 (0.0674)  time: 0.3618  data: 0.0378  max mem: 2311\n","Epoch: [0]  [210/730]  eta: 0:03:07  lr: 0.001451  loss: 1.1587 (1.4215)  loss_classifier: 0.2606 (0.3022)  loss_box_reg: 0.3934 (0.3456)  loss_mask: 0.3302 (0.4241)  loss_objectness: 0.0873 (0.2826)  loss_rpn_box_reg: 0.0589 (0.0670)  time: 0.3687  data: 0.0362  max mem: 2311\n","Epoch: [0]  [220/730]  eta: 0:03:04  lr: 0.001519  loss: 1.1886 (1.4114)  loss_classifier: 0.2624 (0.3001)  loss_box_reg: 0.3933 (0.3493)  loss_mask: 0.3076 (0.4199)  loss_objectness: 0.0873 (0.2749)  loss_rpn_box_reg: 0.0533 (0.0671)  time: 0.3751  data: 0.0373  max mem: 2318\n","Epoch: [0]  [230/730]  eta: 0:03:00  lr: 0.001588  loss: 1.1782 (1.4011)  loss_classifier: 0.2617 (0.2996)  loss_box_reg: 0.3727 (0.3510)  loss_mask: 0.3330 (0.4161)  loss_objectness: 0.0830 (0.2676)  loss_rpn_box_reg: 0.0650 (0.0668)  time: 0.3720  data: 0.0393  max mem: 2321\n","Epoch: [0]  [240/730]  eta: 0:02:57  lr: 0.001656  loss: 1.1577 (1.3889)  loss_classifier: 0.2617 (0.2979)  loss_box_reg: 0.3850 (0.3528)  loss_mask: 0.3330 (0.4122)  loss_objectness: 0.0905 (0.2600)  loss_rpn_box_reg: 0.0492 (0.0660)  time: 0.3683  data: 0.0371  max mem: 2321\n","Epoch: [0]  [250/730]  eta: 0:02:53  lr: 0.001725  loss: 1.0471 (1.3731)  loss_classifier: 0.2569 (0.2961)  loss_box_reg: 0.3624 (0.3507)  loss_mask: 0.3284 (0.4085)  loss_objectness: 0.0765 (0.2528)  loss_rpn_box_reg: 0.0336 (0.0650)  time: 0.3575  data: 0.0339  max mem: 2321\n","Epoch: [0]  [260/730]  eta: 0:02:49  lr: 0.001793  loss: 1.0241 (1.3578)  loss_classifier: 0.2297 (0.2933)  loss_box_reg: 0.2744 (0.3488)  loss_mask: 0.3245 (0.4051)  loss_objectness: 0.0632 (0.2464)  loss_rpn_box_reg: 0.0387 (0.0643)  time: 0.3478  data: 0.0318  max mem: 2321\n","Epoch: [0]  [270/730]  eta: 0:02:46  lr: 0.001862  loss: 1.0373 (1.3531)  loss_classifier: 0.2468 (0.2918)  loss_box_reg: 0.3688 (0.3499)  loss_mask: 0.3064 (0.4016)  loss_objectness: 0.0813 (0.2450)  loss_rpn_box_reg: 0.0423 (0.0647)  time: 0.3605  data: 0.0330  max mem: 2321\n","Epoch: [0]  [280/730]  eta: 0:02:42  lr: 0.001930  loss: 1.1658 (1.3446)  loss_classifier: 0.2558 (0.2907)  loss_box_reg: 0.3822 (0.3506)  loss_mask: 0.3185 (0.3986)  loss_objectness: 0.1108 (0.2403)  loss_rpn_box_reg: 0.0467 (0.0644)  time: 0.3715  data: 0.0347  max mem: 2322\n","Epoch: [0]  [290/730]  eta: 0:02:39  lr: 0.001999  loss: 1.1658 (1.3395)  loss_classifier: 0.2443 (0.2891)  loss_box_reg: 0.4026 (0.3520)  loss_mask: 0.3174 (0.3962)  loss_objectness: 0.1160 (0.2378)  loss_rpn_box_reg: 0.0512 (0.0644)  time: 0.3766  data: 0.0363  max mem: 2322\n","Epoch: [0]  [300/730]  eta: 0:02:35  lr: 0.002067  loss: 1.1242 (1.3315)  loss_classifier: 0.2162 (0.2875)  loss_box_reg: 0.3155 (0.3516)  loss_mask: 0.3103 (0.3945)  loss_objectness: 0.0938 (0.2331)  loss_rpn_box_reg: 0.0508 (0.0648)  time: 0.3694  data: 0.0354  max mem: 2322\n","Epoch: [0]  [310/730]  eta: 0:02:32  lr: 0.002136  loss: 1.0958 (1.3262)  loss_classifier: 0.2355 (0.2867)  loss_box_reg: 0.3441 (0.3535)  loss_mask: 0.3194 (0.3927)  loss_objectness: 0.0823 (0.2289)  loss_rpn_box_reg: 0.0342 (0.0644)  time: 0.3689  data: 0.0353  max mem: 2322\n","Epoch: [0]  [320/730]  eta: 0:02:28  lr: 0.002204  loss: 1.0206 (1.3132)  loss_classifier: 0.2376 (0.2844)  loss_box_reg: 0.3551 (0.3513)  loss_mask: 0.3101 (0.3900)  loss_objectness: 0.0816 (0.2240)  loss_rpn_box_reg: 0.0334 (0.0634)  time: 0.3686  data: 0.0383  max mem: 2322\n","Epoch: [0]  [330/730]  eta: 0:02:25  lr: 0.002273  loss: 0.9408 (1.3016)  loss_classifier: 0.2207 (0.2824)  loss_box_reg: 0.3252 (0.3500)  loss_mask: 0.2975 (0.3878)  loss_objectness: 0.0639 (0.2191)  loss_rpn_box_reg: 0.0284 (0.0623)  time: 0.3610  data: 0.0412  max mem: 2322\n","Epoch: [0]  [340/730]  eta: 0:02:21  lr: 0.002341  loss: 1.0952 (1.2999)  loss_classifier: 0.2664 (0.2833)  loss_box_reg: 0.3517 (0.3525)  loss_mask: 0.3102 (0.3863)  loss_objectness: 0.0681 (0.2161)  loss_rpn_box_reg: 0.0364 (0.0618)  time: 0.3736  data: 0.0444  max mem: 2322\n","Epoch: [0]  [350/730]  eta: 0:02:18  lr: 0.002410  loss: 1.1811 (1.2965)  loss_classifier: 0.2799 (0.2831)  loss_box_reg: 0.3938 (0.3540)  loss_mask: 0.3162 (0.3846)  loss_objectness: 0.0814 (0.2127)  loss_rpn_box_reg: 0.0464 (0.0621)  time: 0.3787  data: 0.0449  max mem: 2325\n","Epoch: [0]  [360/730]  eta: 0:02:14  lr: 0.002479  loss: 1.0931 (1.2912)  loss_classifier: 0.2488 (0.2817)  loss_box_reg: 0.3715 (0.3545)  loss_mask: 0.3471 (0.3840)  loss_objectness: 0.0805 (0.2092)  loss_rpn_box_reg: 0.0444 (0.0617)  time: 0.3671  data: 0.0438  max mem: 2325\n","Epoch: [0]  [370/730]  eta: 0:02:10  lr: 0.002547  loss: 1.1009 (1.2851)  loss_classifier: 0.2349 (0.2804)  loss_box_reg: 0.3452 (0.3544)  loss_mask: 0.3383 (0.3824)  loss_objectness: 0.0899 (0.2062)  loss_rpn_box_reg: 0.0471 (0.0617)  time: 0.3673  data: 0.0419  max mem: 2325\n","Epoch: [0]  [380/730]  eta: 0:02:07  lr: 0.002616  loss: 1.0106 (1.2791)  loss_classifier: 0.2446 (0.2796)  loss_box_reg: 0.3299 (0.3547)  loss_mask: 0.3178 (0.3811)  loss_objectness: 0.0745 (0.2027)  loss_rpn_box_reg: 0.0440 (0.0611)  time: 0.3709  data: 0.0380  max mem: 2325\n","Epoch: [0]  [390/730]  eta: 0:02:03  lr: 0.002684  loss: 1.0762 (1.2766)  loss_classifier: 0.2559 (0.2796)  loss_box_reg: 0.3751 (0.3558)  loss_mask: 0.3403 (0.3800)  loss_objectness: 0.0876 (0.2001)  loss_rpn_box_reg: 0.0411 (0.0610)  time: 0.3673  data: 0.0354  max mem: 2325\n","Epoch: [0]  [400/730]  eta: 0:02:00  lr: 0.002753  loss: 1.1165 (1.2708)  loss_classifier: 0.2520 (0.2788)  loss_box_reg: 0.3751 (0.3561)  loss_mask: 0.3289 (0.3783)  loss_objectness: 0.0912 (0.1970)  loss_rpn_box_reg: 0.0441 (0.0607)  time: 0.3626  data: 0.0336  max mem: 2325\n","Epoch: [0]  [410/730]  eta: 0:01:56  lr: 0.002821  loss: 1.0133 (1.2658)  loss_classifier: 0.2323 (0.2778)  loss_box_reg: 0.3677 (0.3570)  loss_mask: 0.2932 (0.3763)  loss_objectness: 0.0593 (0.1943)  loss_rpn_box_reg: 0.0441 (0.0604)  time: 0.3663  data: 0.0346  max mem: 2326\n","Epoch: [0]  [420/730]  eta: 0:01:52  lr: 0.002890  loss: 1.0593 (1.2640)  loss_classifier: 0.2333 (0.2772)  loss_box_reg: 0.3802 (0.3583)  loss_mask: 0.3185 (0.3754)  loss_objectness: 0.0871 (0.1923)  loss_rpn_box_reg: 0.0441 (0.0607)  time: 0.3723  data: 0.0370  max mem: 2326\n","Epoch: [0]  [430/730]  eta: 0:01:49  lr: 0.002958  loss: 1.1734 (1.2619)  loss_classifier: 0.2538 (0.2774)  loss_box_reg: 0.4087 (0.3596)  loss_mask: 0.3262 (0.3741)  loss_objectness: 0.0906 (0.1900)  loss_rpn_box_reg: 0.0514 (0.0608)  time: 0.3751  data: 0.0396  max mem: 2326\n","Epoch: [0]  [440/730]  eta: 0:01:45  lr: 0.003027  loss: 1.0679 (1.2570)  loss_classifier: 0.2526 (0.2770)  loss_box_reg: 0.3581 (0.3598)  loss_mask: 0.3023 (0.3727)  loss_objectness: 0.0816 (0.1872)  loss_rpn_box_reg: 0.0404 (0.0602)  time: 0.3739  data: 0.0394  max mem: 2326\n","Epoch: [0]  [450/730]  eta: 0:01:42  lr: 0.003095  loss: 1.1151 (1.2548)  loss_classifier: 0.2569 (0.2767)  loss_box_reg: 0.4121 (0.3609)  loss_mask: 0.3010 (0.3712)  loss_objectness: 0.0807 (0.1854)  loss_rpn_box_reg: 0.0372 (0.0606)  time: 0.3799  data: 0.0419  max mem: 2326\n","Epoch: [0]  [460/730]  eta: 0:01:38  lr: 0.003164  loss: 1.1151 (1.2497)  loss_classifier: 0.2569 (0.2758)  loss_box_reg: 0.4121 (0.3609)  loss_mask: 0.3032 (0.3703)  loss_objectness: 0.0762 (0.1827)  loss_rpn_box_reg: 0.0446 (0.0601)  time: 0.3795  data: 0.0439  max mem: 2326\n","Epoch: [0]  [470/730]  eta: 0:01:35  lr: 0.003232  loss: 1.0722 (1.2470)  loss_classifier: 0.2212 (0.2755)  loss_box_reg: 0.3883 (0.3621)  loss_mask: 0.2989 (0.3689)  loss_objectness: 0.0542 (0.1807)  loss_rpn_box_reg: 0.0360 (0.0598)  time: 0.3704  data: 0.0414  max mem: 2326\n","Epoch: [0]  [480/730]  eta: 0:01:31  lr: 0.003301  loss: 1.0933 (1.2435)  loss_classifier: 0.2423 (0.2749)  loss_box_reg: 0.4273 (0.3629)  loss_mask: 0.2989 (0.3677)  loss_objectness: 0.0657 (0.1785)  loss_rpn_box_reg: 0.0392 (0.0595)  time: 0.3736  data: 0.0418  max mem: 2326\n","Epoch: [0]  [490/730]  eta: 0:01:27  lr: 0.003369  loss: 1.0543 (1.2402)  loss_classifier: 0.2446 (0.2744)  loss_box_reg: 0.3944 (0.3632)  loss_mask: 0.2975 (0.3662)  loss_objectness: 0.0686 (0.1770)  loss_rpn_box_reg: 0.0411 (0.0593)  time: 0.3781  data: 0.0428  max mem: 2326\n","Epoch: [0]  [500/730]  eta: 0:01:24  lr: 0.003438  loss: 0.9987 (1.2355)  loss_classifier: 0.2389 (0.2734)  loss_box_reg: 0.3388 (0.3635)  loss_mask: 0.2890 (0.3649)  loss_objectness: 0.0635 (0.1750)  loss_rpn_box_reg: 0.0363 (0.0588)  time: 0.3757  data: 0.0399  max mem: 2326\n","Epoch: [0]  [510/730]  eta: 0:01:20  lr: 0.003506  loss: 1.0104 (1.2349)  loss_classifier: 0.2207 (0.2729)  loss_box_reg: 0.3874 (0.3645)  loss_mask: 0.2890 (0.3641)  loss_objectness: 0.0645 (0.1742)  loss_rpn_box_reg: 0.0363 (0.0592)  time: 0.3751  data: 0.0388  max mem: 2326\n","Epoch: [0]  [520/730]  eta: 0:01:16  lr: 0.003575  loss: 0.9910 (1.2303)  loss_classifier: 0.2417 (0.2726)  loss_box_reg: 0.3297 (0.3635)  loss_mask: 0.2828 (0.3627)  loss_objectness: 0.0808 (0.1727)  loss_rpn_box_reg: 0.0315 (0.0588)  time: 0.3765  data: 0.0411  max mem: 2326\n","Epoch: [0]  [530/730]  eta: 0:01:13  lr: 0.003643  loss: 0.9910 (1.2277)  loss_classifier: 0.2568 (0.2727)  loss_box_reg: 0.3297 (0.3638)  loss_mask: 0.2972 (0.3618)  loss_objectness: 0.0716 (0.1709)  loss_rpn_box_reg: 0.0272 (0.0584)  time: 0.3816  data: 0.0434  max mem: 2326\n","Epoch: [0]  [540/730]  eta: 0:01:09  lr: 0.003712  loss: 1.1709 (1.2271)  loss_classifier: 0.2966 (0.2728)  loss_box_reg: 0.4428 (0.3654)  loss_mask: 0.2989 (0.3606)  loss_objectness: 0.0740 (0.1698)  loss_rpn_box_reg: 0.0334 (0.0585)  time: 0.3899  data: 0.0445  max mem: 2326\n","Epoch: [0]  [550/730]  eta: 0:01:06  lr: 0.003780  loss: 1.1150 (1.2230)  loss_classifier: 0.2613 (0.2715)  loss_box_reg: 0.3738 (0.3645)  loss_mask: 0.2926 (0.3598)  loss_objectness: 0.0740 (0.1689)  loss_rpn_box_reg: 0.0334 (0.0583)  time: 0.3734  data: 0.0409  max mem: 2326\n","Epoch: [0]  [560/730]  eta: 0:01:02  lr: 0.003849  loss: 0.9321 (1.2205)  loss_classifier: 0.2014 (0.2709)  loss_box_reg: 0.2989 (0.3638)  loss_mask: 0.2866 (0.3585)  loss_objectness: 0.0838 (0.1685)  loss_rpn_box_reg: 0.0424 (0.0588)  time: 0.3631  data: 0.0400  max mem: 2326\n","Epoch: [0]  [570/730]  eta: 0:00:58  lr: 0.003917  loss: 0.9670 (1.2182)  loss_classifier: 0.2187 (0.2705)  loss_box_reg: 0.3313 (0.3639)  loss_mask: 0.2866 (0.3577)  loss_objectness: 0.0886 (0.1673)  loss_rpn_box_reg: 0.0495 (0.0588)  time: 0.3690  data: 0.0390  max mem: 2326\n","Epoch: [0]  [580/730]  eta: 0:00:55  lr: 0.003986  loss: 1.0118 (1.2154)  loss_classifier: 0.2187 (0.2699)  loss_box_reg: 0.3623 (0.3645)  loss_mask: 0.2961 (0.3569)  loss_objectness: 0.0632 (0.1655)  loss_rpn_box_reg: 0.0421 (0.0585)  time: 0.3743  data: 0.0359  max mem: 2326\n","Epoch: [0]  [590/730]  eta: 0:00:51  lr: 0.004054  loss: 1.1285 (1.2155)  loss_classifier: 0.2618 (0.2703)  loss_box_reg: 0.4251 (0.3659)  loss_mask: 0.3033 (0.3565)  loss_objectness: 0.0756 (0.1644)  loss_rpn_box_reg: 0.0384 (0.0584)  time: 0.3785  data: 0.0373  max mem: 2326\n","Epoch: [0]  [600/730]  eta: 0:00:47  lr: 0.004123  loss: 1.1472 (1.2128)  loss_classifier: 0.2674 (0.2698)  loss_box_reg: 0.3807 (0.3661)  loss_mask: 0.3133 (0.3558)  loss_objectness: 0.0808 (0.1629)  loss_rpn_box_reg: 0.0376 (0.0582)  time: 0.3758  data: 0.0392  max mem: 2326\n","Epoch: [0]  [610/730]  eta: 0:00:44  lr: 0.004191  loss: 1.0251 (1.2110)  loss_classifier: 0.2409 (0.2700)  loss_box_reg: 0.3807 (0.3663)  loss_mask: 0.2917 (0.3550)  loss_objectness: 0.0781 (0.1619)  loss_rpn_box_reg: 0.0361 (0.0579)  time: 0.3706  data: 0.0374  max mem: 2326\n","Epoch: [0]  [620/730]  eta: 0:00:40  lr: 0.004260  loss: 1.1028 (1.2101)  loss_classifier: 0.2678 (0.2703)  loss_box_reg: 0.4113 (0.3668)  loss_mask: 0.2900 (0.3542)  loss_objectness: 0.0927 (0.1608)  loss_rpn_box_reg: 0.0489 (0.0580)  time: 0.3766  data: 0.0415  max mem: 2326\n","Epoch: [0]  [630/730]  eta: 0:00:36  lr: 0.004329  loss: 1.1028 (1.2080)  loss_classifier: 0.2532 (0.2694)  loss_box_reg: 0.3671 (0.3662)  loss_mask: 0.3042 (0.3536)  loss_objectness: 0.0790 (0.1603)  loss_rpn_box_reg: 0.0432 (0.0586)  time: 0.3723  data: 0.0417  max mem: 2326\n","Epoch: [0]  [640/730]  eta: 0:00:33  lr: 0.004397  loss: 1.0906 (1.2067)  loss_classifier: 0.2212 (0.2691)  loss_box_reg: 0.3671 (0.3663)  loss_mask: 0.3016 (0.3527)  loss_objectness: 0.0984 (0.1598)  loss_rpn_box_reg: 0.0432 (0.0587)  time: 0.3694  data: 0.0402  max mem: 2326\n","Epoch: [0]  [650/730]  eta: 0:00:29  lr: 0.004466  loss: 1.0249 (1.2035)  loss_classifier: 0.2440 (0.2687)  loss_box_reg: 0.3681 (0.3661)  loss_mask: 0.2815 (0.3516)  loss_objectness: 0.0966 (0.1587)  loss_rpn_box_reg: 0.0451 (0.0584)  time: 0.3778  data: 0.0432  max mem: 2326\n","Epoch: [0]  [660/730]  eta: 0:00:25  lr: 0.004534  loss: 0.9830 (1.2018)  loss_classifier: 0.2340 (0.2683)  loss_box_reg: 0.3453 (0.3657)  loss_mask: 0.3011 (0.3510)  loss_objectness: 0.0705 (0.1582)  loss_rpn_box_reg: 0.0347 (0.0586)  time: 0.3707  data: 0.0399  max mem: 2326\n","Epoch: [0]  [670/730]  eta: 0:00:22  lr: 0.004603  loss: 1.1010 (1.1995)  loss_classifier: 0.2340 (0.2678)  loss_box_reg: 0.3453 (0.3658)  loss_mask: 0.3011 (0.3502)  loss_objectness: 0.0820 (0.1573)  loss_rpn_box_reg: 0.0352 (0.0585)  time: 0.3726  data: 0.0391  max mem: 2326\n","Epoch: [0]  [680/730]  eta: 0:00:18  lr: 0.004671  loss: 1.0918 (1.1973)  loss_classifier: 0.2339 (0.2673)  loss_box_reg: 0.3227 (0.3654)  loss_mask: 0.3029 (0.3499)  loss_objectness: 0.0817 (0.1565)  loss_rpn_box_reg: 0.0397 (0.0582)  time: 0.3730  data: 0.0408  max mem: 2326\n","Epoch: [0]  [690/730]  eta: 0:00:14  lr: 0.004740  loss: 1.0918 (1.1946)  loss_classifier: 0.2228 (0.2666)  loss_box_reg: 0.3079 (0.3647)  loss_mask: 0.3070 (0.3493)  loss_objectness: 0.0832 (0.1556)  loss_rpn_box_reg: 0.0351 (0.0583)  time: 0.3668  data: 0.0391  max mem: 2326\n","Epoch: [0]  [700/730]  eta: 0:00:11  lr: 0.004808  loss: 1.0083 (1.1909)  loss_classifier: 0.2263 (0.2660)  loss_box_reg: 0.3496 (0.3639)  loss_mask: 0.2929 (0.3485)  loss_objectness: 0.0753 (0.1545)  loss_rpn_box_reg: 0.0351 (0.0581)  time: 0.3691  data: 0.0386  max mem: 2326\n","Epoch: [0]  [710/730]  eta: 0:00:07  lr: 0.004877  loss: 1.0456 (1.1892)  loss_classifier: 0.2435 (0.2658)  loss_box_reg: 0.3619 (0.3643)  loss_mask: 0.2916 (0.3477)  loss_objectness: 0.0753 (0.1534)  loss_rpn_box_reg: 0.0414 (0.0579)  time: 0.3781  data: 0.0406  max mem: 2326\n","Epoch: [0]  [720/730]  eta: 0:00:03  lr: 0.004945  loss: 1.0054 (1.1870)  loss_classifier: 0.2306 (0.2652)  loss_box_reg: 0.3688 (0.3641)  loss_mask: 0.2674 (0.3466)  loss_objectness: 0.0796 (0.1532)  loss_rpn_box_reg: 0.0414 (0.0580)  time: 0.3830  data: 0.0417  max mem: 2326\n","Epoch: [0]  [729/730]  eta: 0:00:00  lr: 0.005000  loss: 1.0647 (1.1861)  loss_classifier: 0.2336 (0.2651)  loss_box_reg: 0.3688 (0.3646)  loss_mask: 0.2918 (0.3461)  loss_objectness: 0.0881 (0.1524)  loss_rpn_box_reg: 0.0389 (0.0578)  time: 0.3694  data: 0.0425  max mem: 2326\n","Epoch: [0] Total time: 0:04:29 (0.3685 s / it)\n","creating index...\n","index created!\n","Test:  [  0/163]  eta: 0:00:33  model_time: 0.1336 (0.1336)  evaluator_time: 0.0505 (0.0505)  time: 0.2061  data: 0.0220  max mem: 2326\n","Test:  [100/163]  eta: 0:00:11  model_time: 0.1142 (0.1140)  evaluator_time: 0.0474 (0.0489)  time: 0.1860  data: 0.0202  max mem: 2326\n","Test:  [162/163]  eta: 0:00:00  model_time: 0.1187 (0.1148)  evaluator_time: 0.0590 (0.0491)  time: 0.2056  data: 0.0232  max mem: 2326\n","Test: Total time: 0:00:30 (0.1864 s / it)\n","Averaged stats: model_time: 0.1187 (0.1148)  evaluator_time: 0.0590 (0.0491)\n","Accumulating evaluation results...\n","DONE (t=0.07s).\n","Accumulating evaluation results...\n","DONE (t=0.06s).\n","IoU metric: bbox\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.056\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.131\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.033\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.051\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.055\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.128\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.025\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.119\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.201\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.179\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.224\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.339\n","IoU metric: segm\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.056\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.134\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.034\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.043\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.068\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.208\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.023\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.117\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.194\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.177\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.210\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.305\n","<coco_eval.CocoEvaluator object at 0x000001BF68546910>\n","That's it!\n"]}],"source":["# get the model using our helper function\n","model = get_model_instance_segmentation(num_classes)\n","\n","# move model to the right device\n","model.to(device)\n","\n","# construct an optimizer\n","params = [p for p in model.parameters() if p.requires_grad]\n","# optimizer = torch.optim.Adam(model.parameters(), lr=0.0003, weight_decay=1e-5)\n","optimizer = torch.optim.SGD(params, lr=0.005,\n","                            momentum=0.9, weight_decay=0.0005)\n","# and a learning rate scheduler\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n","                                                step_size=3,\n","                                                gamma=0.1)\n","\n","# let's train it for 10 epochs\n","num_epochs = 20\n","#  for batch in tqdm(train_loader):\n","for epoch in tqdm(range(num_epochs)):\n","    # print(\"hi\")\n","    # train for one epoch, printing every 10 iterations\n","    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n","    # update the learning rate\n","    lr_scheduler.step()\n","    # evaluate on the test dataset\n","    result = evaluate(model, data_loader_test, device=device)\n","    # print(result)\n","    # torch.save(model.state_dict(), f'./output/epoch{epoch}.ckpt')\n","\n","    # # pick one image from the test set\n","    # img, target = dataset_test[0]\n","\n","    # # put the model in evaluation mode\n","    # model.eval()\n","    # with torch.no_grad():\n","    #     prediction = model([img.to(device)])\n","    # Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())\n","    # Image.fromarray(prediction[0]['masks'][0, 0].mul(255).byte().cpu().numpy())\n","\n","print(\"That's it!\")"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"491e617db30040a9a84b439c0e9d4614","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/163 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"KeyError","evalue":"'image_path'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[1;32mIn[11], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m img1 \u001b[39m=\u001b[39m img_showmask(image, mask)\n\u001b[0;32m     18\u001b[0m cv2\u001b[39m.\u001b[39mimshow(\u001b[39m'\u001b[39m\u001b[39m1\u001b[39m\u001b[39m'\u001b[39m,img1)\n\u001b[1;32m---> 20\u001b[0m image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(target[\u001b[39m\"\u001b[39;49m\u001b[39mimage_path\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m     21\u001b[0m result \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(all_mask\u001b[39m.\u001b[39mmul(\u001b[39m255\u001b[39m)\u001b[39m.\u001b[39mbyte()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n\u001b[0;32m     22\u001b[0m result \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(np\u001b[39m.\u001b[39masarray(result),cv2\u001b[39m.\u001b[39mCOLOR_RGB2BGR)  \n","\u001b[1;31mKeyError\u001b[0m: 'image_path'"]}],"source":["# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","# model_best = get_model_instance_segmentation(num_classes=2).to(device)\n","\n","# model_best.load_state_dict(torch.load(\"output/epoch19.ckpt\"))\n","# model_best.eval()\n","# with torch.no_grad():\n","#     for img, target, path in tqdm(dataset_test):\n","#         prediction = model_best([img.to(device)])\n","\n","#         all_mask = torch.zeros(prediction[0]['masks'][0,0].size())\n","#         for i in prediction[0]['masks']:\n","#             all_mask += i[0].cpu()\n","\n","#         mask = cv2.imread(path[\"mask_path\"])\n","#         mask = cv2.cvtColor(np.asarray(mask),cv2.COLOR_RGB2BGR) \n","#         image = cv2.imread(path[\"image_path\"])\n","#         img1 = img_showmask(image, mask)\n","#         cv2.imshow('1',img1)\n","        \n","#         image = cv2.imread(target[\"image_path\"])\n","#         result = Image.fromarray(all_mask.mul(255).byte().cpu().numpy())\n","#         result = cv2.cvtColor(np.asarray(result),cv2.COLOR_RGB2BGR)  \n","#         img2 = img_showmask(image, result)\n","#         cv2.imshow('2',img2)\n","#         cv2.waitKey()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":4}
