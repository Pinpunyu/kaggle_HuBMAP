{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import sys\n","sys.path.append(\"./detection\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import numpy as np\n","import torch\n","from PIL import Image\n","from pathlib import Path\n","import cv2\n","from engine import train_one_epoch, evaluate\n","from data_process import img_showmask\n","import utils\n","import torch.utils.data as data\n","from tqdm.auto import tqdm\n","import matplotlib.pyplot as plt\n","\n","\n","class PennFudanDataset(torch.utils.data.Dataset):\n","\n","    def __init__(self, root, transforms):\n","        self.root = root\n","        self.transforms = transforms\n","        # load all image files, sorting them to\n","        # ensure that they are aligned\n","        self.imgs = list(sorted(os.listdir(os.path.join(root, \"image\"))))\n","        self.masks = list(sorted(os.listdir(os.path.join(root, \"mask\"))))\n","\n","\n","    def __getitem__(self, idx):\n","        # load images and masks\n","        img_path = os.path.join(self.root, \"image\", self.imgs[idx])\n","        mask_path = os.path.join(self.root, \"mask\", self.masks[idx])\n","        # print(img_path)\n","        # img_path = self.imgs[idx]\n","        # mask_path = self.masks[idx]\n","        img = Image.open(img_path).convert(\"RGB\")\n","        # note that we haven't converted the mask to RGB,\n","        # because each color corresponds to a different instance\n","        # with 0 being background\n","        mask = Image.open(mask_path).convert('L')\n","        # convert the PIL Image into a numpy array\n","        mask = np.array(mask)\n","        # instances are encoded as different colors\n","        obj_ids = np.unique(mask)\n","        # first id is the background, so remove it\n","        \n","        obj_ids = obj_ids[1:]\n","        # print((obj_ids[:, None, None]).shape)\n","        # print(obj_ids)\n","        # split the color-encoded mask into a set\n","        # of binary masks\n","        masks = mask == obj_ids[:, None, None]\n","        \n","        # print(masks)\n","        \n","\n","        # get bounding box coordinates for each mask\n","        num_objs = len(obj_ids)\n","        boxes = []\n","        for i in range(num_objs):\n","            pos = np.nonzero(masks[i])\n","            xmin = np.min(pos[1])\n","            xmax = np.max(pos[1])\n","            ymin = np.min(pos[0])\n","            ymax = np.max(pos[0])\n","            boxes.append([xmin, ymin, xmax, ymax])\n","\n","        # convert everything into a torch.Tensor\n","        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","        # there is only one class\n","        labels = torch.ones((num_objs,), dtype=torch.int64)\n","        masks = torch.as_tensor(masks, dtype=torch.uint8)\n","        image_id = torch.tensor([idx])\n","\n","        \n","        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n","    \n","        # suppose all instances are not crowd\n","        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n","\n","        target = {}\n","        target[\"boxes\"] = boxes\n","        target[\"labels\"] = labels\n","        target[\"masks\"] = masks\n","        target[\"image_id\"] = image_id\n","        target[\"area\"] = area\n","        target[\"iscrowd\"] = iscrowd\n","\n","        path = {}\n","        path[\"mask_path\"] = mask_path\n","        path[\"image_path\"] = img_path\n","\n","        if self.transforms is not None:\n","            img, target = self.transforms(img, target)\n","\n","        return img, target, path\n","\n","    def __len__(self):\n","        return len(self.imgs)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torchvision\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","\n","# load a model pre-trained on COCO\n","model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n","\n","# replace the classifier with a new one, that has\n","# num_classes which is user-defined\n","num_classes = 2  # 1 class (person) + background\n","# get number of input features for the classifier\n","in_features = model.roi_heads.box_predictor.cls_score.in_features\n","# replace the pre-trained head with a new one\n","model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torchvision\n","from torchvision.models.detection import FasterRCNN\n","from torchvision.models.detection.rpn import AnchorGenerator\n","\n","# load a pre-trained model for classification and return\n","# only the features\n","backbone = torchvision.models.mobilenet_v2(weights=\"DEFAULT\").features\n","# FasterRCNN needs to know the number of\n","# output channels in a backbone. For mobilenet_v2, it's 1280\n","# so we need to add it here\n","backbone.out_channels = 1280\n","\n","# let's make the RPN generate 5 x 3 anchors per spatial\n","# location, with 5 different sizes and 3 different aspect\n","# ratios. We have a Tuple[Tuple[int]] because each feature\n","# map could potentially have different sizes and\n","# aspect ratios\n","anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n","                                   aspect_ratios=((0.5, 1.0, 2.0),))\n","\n","# let's define what are the feature maps that we will\n","# use to perform the region of interest cropping, as well as\n","# the size of the crop after rescaling.\n","# if your backbone returns a Tensor, featmap_names is expected to\n","# be [0]. More generally, the backbone should return an\n","# OrderedDict[Tensor], and in featmap_names you can choose which\n","# feature maps to use.\n","roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n","                                                output_size=7,\n","                                                sampling_ratio=2)\n","\n","# put the pieces together inside a FasterRCNN model\n","model = FasterRCNN(backbone,\n","                   num_classes=2,\n","                   rpn_anchor_generator=anchor_generator,\n","                   box_roi_pool=roi_pooler)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torchvision\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n","\n","\n","def get_model_instance_segmentation(num_classes):\n","    # load an instance segmentation model pre-trained on COCO\n","    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n","\n","    # get number of input features for the classifier\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    # replace the pre-trained head with a new one\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","    # now get the number of input features for the mask classifier\n","    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n","    hidden_layer = 256\n","    # and replace the mask predictor with a new one\n","    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n","                                                       hidden_layer,\n","                                                       num_classes)\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import transforms as T\n","\n","def get_transform(train):\n","    transforms = []\n","    transforms.append(T.PILToTensor())\n","    transforms.append(T.ConvertImageDtype(torch.float))\n","    if train:\n","        transforms.append(T.RandomHorizontalFlip(0.5))\n","    return T.Compose(transforms)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# from engine import train_one_epoch, evaluate\n","# import utils\n","# import torch.utils.data as data\n","# from tqdm.auto import tqdm\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n","# dataset = PennFudanDataset('./data/hubmap-hacking-the-human-vasculature/train', get_transform(train=True))\n","# data_loader = torch.utils.data.DataLoader(\n","#  dataset, batch_size=2, shuffle=True, num_workers=0,\n","#  collate_fn=utils.collate_fn)\n","# # For Training\n","# images,targets = next(iter(data_loader))\n","# images = list(image for image in images)\n","# targets = [{k: v for k, v in t.items()} for t in targets]\n","# output = model(images,targets)   # Returns losses and detections\n","# # For inference\n","# model.eval()\n","# x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n","# predictions = model(x)           # Returns predictions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# train on the GPU or on the CPU, if a GPU is not available\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","# our dataset has two classes only - background and person\n","num_classes = 2\n","# use our dataset and defined transformations\n","\n","dataset = PennFudanDataset('./data/hubmap-hacking-the-human-vasculature/train', get_transform(train=True))\n","dataset_test = PennFudanDataset('./data/hubmap-hacking-the-human-vasculature/train', get_transform(train=False))\n","\n","train_size=int(len(dataset)*0.9)\n","validation_size=len(dataset)-train_size\n","dataset,dataset_test=data.random_split(dataset, [train_size,validation_size])\n","\n","\n","# split the dataset in train and test set\n","# indices = torch.randperm(len(dataset)).tolist()\n","# dataset = torch.utils.data.Subset(dataset, indices[:-50])\n","# dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n","\n","# define training and validation data loaders\n","data_loader = torch.utils.data.DataLoader(\n","    dataset, batch_size=2, shuffle=True, num_workers=0,\n","    collate_fn=utils.collate_fn)\n","\n","data_loader_test = torch.utils.data.DataLoader(\n","    dataset_test, batch_size=1, shuffle=False, num_workers=0,\n","    collate_fn=utils.collate_fn)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# get the model using our helper function\n","model = get_model_instance_segmentation(num_classes)\n","\n","# move model to the right device\n","model.to(device)\n","\n","# construct an optimizer\n","params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.0003, weight_decay=1e-5)\n","# optimizer = torch.optim.SGD(params, lr=0.005,\n","                            # momentum=0.9, weight_decay=0.0005)\n","# and a learning rate scheduler\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n","                                                step_size=3,\n","                                                gamma=0.1)\n","\n","# let's train it for 10 epochs\n","num_epochs = 20\n","#  for batch in tqdm(train_loader):\n","for epoch in tqdm(range(num_epochs)):\n","    # print(\"hi\")\n","    # train for one epoch, printing every 10 iterations\n","    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n","    # update the learning rate\n","    lr_scheduler.step()\n","    # evaluate on the test dataset\n","    result = evaluate(model, data_loader_test, device=device)\n","    # print(result)\n","    torch.save(model.state_dict(), f'./model/epoch{epoch}.ckpt')\n","\n","\n","print(\"That's it!\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","model_best = get_model_instance_segmentation(num_classes=2).to(device)\n","\n","model_best.load_state_dict(torch.load(\"output/epoch19.ckpt\"))\n","model_best.eval()\n","with torch.no_grad():\n","    for img, target, path in tqdm(dataset_test):\n","        prediction = model_best([img.to(device)])\n","\n","        all_mask = torch.zeros(prediction[0]['masks'][0,0].size())\n","        for i in prediction[0]['masks']:\n","            all_mask += i[0].cpu()\n","\n","        mask = cv2.imread(path[\"mask_path\"])\n","        mask = cv2.cvtColor(np.asarray(mask),cv2.COLOR_RGB2BGR) \n","        image = cv2.imread(path[\"image_path\"])\n","        img1 = img_showmask(image, mask)\n","        cv2.imshow('1',img1)\n","        \n","        image = cv2.imread(path[\"image_path\"])\n","        result = Image.fromarray(all_mask.mul(255).byte().cpu().numpy())\n","        result = cv2.cvtColor(np.asarray(result),cv2.COLOR_RGB2BGR)  \n","        img2 = img_showmask(image, result)\n","        cv2.imshow('2',img2)\n","        cv2.waitKey()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":4}
